{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUXn947R19oloWV2o6uTmM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luther-exe/Explainable-Federated-Learning-for-Breast-Cancer-Diagnosis/blob/main/XFL_breast_cancer_diagnosis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EzeH9GCtPB5o"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet tensorflow==2.14.0 tensorflow_federated==0.71.0\n",
        "#Restart runtime after installing tensorflow and tensorflow federated before proceeding!!!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import sklearn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import logging\n",
        "import os\n",
        "from IPython.display import Image\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    return rng\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess_data():\n",
        "    logging.info(\"Loading breast cancer dataset...\")\n",
        "    try:\n",
        "        data = load_breast_cancer()\n",
        "        X = data.data\n",
        "        y = data.target\n",
        "        if X.shape[1] != 30:\n",
        "            raise ValueError(f\"Expected 30 features, got {X.shape[1]}\")\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        logging.info(\"Data scaled. Feature names: %s\", data.feature_names)\n",
        "        return X_scaled, y, scaler, data.feature_names\n",
        "    except ValueError as ve:\n",
        "        logging.error(\"ValueError in load_and_preprocess_data: %s\", ve)\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logging.error(\"Unexpected error in load_and_preprocess_data: %s\", e)\n",
        "        raise\n",
        "\n",
        "# Create IID and Non-IID partitions\n",
        "def create_data_partitions(X, y, num_clients=5, non_iid=False, rng=None):\n",
        "    logging.info(\"Creating data partitions...\")\n",
        "    try:\n",
        "        if num_clients <= 0:\n",
        "            raise ValueError(\"num_clients must be positive\")\n",
        "        if len(X) != len(y):\n",
        "            raise ValueError(\"X and y must have the same length\")\n",
        "        if non_iid:\n",
        "            sorted_indices = np.argsort(X[:, 0])\n",
        "            X, y = X[sorted_indices], y[sorted_indices]\n",
        "            client_data = []\n",
        "            split_size = len(X) // num_clients\n",
        "            for i in range(num_clients):\n",
        "                start_idx = i * split_size\n",
        "                end_idx = (i + 1) * split_size if i < num_clients - 1 else len(X)\n",
        "                client_data.append((X[start_idx:end_idx], y[start_idx:end_idx]))\n",
        "        else:\n",
        "            indices = rng.permutation(len(X))\n",
        "            X, y = X[indices] , y[indices]\n",
        "            client_data = []\n",
        "            split_size = len(X) // num_clients\n",
        "            for i in range(num_clients):\n",
        "                start_idx = i * split_size\n",
        "                end_idx = (i + 1) * split_size if i < num_clients - 1 else len(X)\n",
        "                client_data.append((X[start_idx:end_idx], y[start_idx:end_idx]))\n",
        "        logging.info(\"Partitions created.\")\n",
        "        return client_data\n",
        "    except ValueError as ve:\n",
        "        logging.error(\"ValueError in create_data_partitions: %s\", ve)\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logging.error(\"Unexpected error in create_data_partitions: %s\", e)\n",
        "        raise\n",
        "\n",
        "# Convert client data to TFF-compatible datasets\n",
        "def create_tff_datasets(client_data, batch_size=32):\n",
        "    logging.info(\"Converting to TFF datasets...\")\n",
        "    try:\n",
        "        def create_tf_dataset(X, y):\n",
        "            X = X.astype(np.float32)\n",
        "            y = y.astype(np.int32)\n",
        "            dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
        "            return dataset\n",
        "        datasets = [create_tf_dataset(X, y) for X, y in client_data]\n",
        "        for i, ds in enumerate(datasets):\n",
        "            for batch in ds.take(1):\n",
        "                logging.info(\"Client %d dataset batch shape: features=%s, labels=%s\", i+1, batch[0].shape, batch[1].shape)\n",
        "        logging.info(\"TFF datasets created.\")\n",
        "        return datasets\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in create_tff_datasets: %s\", e)\n",
        "        raise\n",
        "\n",
        "# Define neural network model\n",
        "def create_model():\n",
        "    logging.info(\"Creating model...\")\n",
        "    try:\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Input(shape=(30,)),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        logging.info(\"Model created.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in create_model: %s\", e)\n",
        "        raise\n",
        "\n",
        "# TFF model function\n",
        "def model_fn():\n",
        "    logging.info(\"Creating TFF model...\")\n",
        "    try:\n",
        "        keras_model = create_model()\n",
        "        input_spec = tff.types.StructWithPythonType(\n",
        "            (\n",
        "                tff.types.TensorType(dtype=np.float32, shape=[None, 30]),\n",
        "                tff.types.TensorType(dtype=np.int32, shape=[None])\n",
        "            ),\n",
        "            container_type=tuple\n",
        "        )\n",
        "        model = tff.learning.models.from_keras_model(\n",
        "            keras_model=keras_model,\n",
        "            input_spec=input_spec,\n",
        "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "            metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "        )\n",
        "        logging.info(\"TFF model created.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in model_fn: %s\", e)\n",
        "        raise\n",
        "\n",
        "# Training function for federated learning\n",
        "def federated_training(client_datasets, num_rounds=20, clients_per_round=3, rng=None):\n",
        "    logging.info(\"Starting federated training...\")\n",
        "    try:\n",
        "        iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "            model_fn,\n",
        "            client_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.001),\n",
        "            server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0)\n",
        "        )\n",
        "        state = iterative_process.initialize()\n",
        "        metrics_history = {'loss': [], 'accuracy': []}\n",
        "        for round_num in range(num_rounds):\n",
        "            sampled_clients = rng.choice(len(client_datasets), clients_per_round, replace=False)\n",
        "            sampled_datasets = [client_datasets[i] for i in sampled_clients]\n",
        "            state, metrics = iterative_process.next(state, sampled_datasets)\n",
        "            logging.info(\"Round %d: loss=%.4f, accuracy=%.4f\",\n",
        "                         round_num + 1,\n",
        "                         metrics['client_work']['train']['loss'],\n",
        "                         metrics['client_work']['train']['binary_accuracy'])\n",
        "            metrics_history['loss'].append(metrics['client_work']['train']['loss'])\n",
        "            metrics_history['accuracy'].append(metrics['client_work']['train']['binary_accuracy'])\n",
        "        logging.info(\"Federated training completed.\")\n",
        "        return state, metrics_history\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in federated_training: %s\", e)\n",
        "        raise\n",
        "\n",
        "# Apply global model weights to a Keras model\n",
        "def apply_global_weights(state, keras_model):\n",
        "    logging.info(\"Applying global weights...\")\n",
        "    try:\n",
        "        model_weights = tff.learning.models.ModelWeights(\n",
        "            trainable=state.global_model_weights.trainable,\n",
        "            non_trainable=state.global_model_weights.non_trainable\n",
        "        )\n",
        "        keras_model.set_weights(model_weights.trainable)\n",
        "        logging.info(\"Global weights applied.\")\n",
        "        return keras_model\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in apply_global_weights: %s\", e)\n",
        "        raise\n",
        "\n",
        "# Personalized federated learning\n",
        "def personalized_federated_learning(state, client_datasets, num_epochs=2):\n",
        "    logging.info(\"Starting personalized federated learning...\")\n",
        "    try:\n",
        "        global_model = create_model()\n",
        "        global_model = apply_global_weights(state, global_model)\n",
        "        personalized_models = []\n",
        "        for client_data in client_datasets:\n",
        "            model = create_model()\n",
        "            model.set_weights(global_model.get_weights())\n",
        "            model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "                          loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                          metrics=['accuracy'])\n",
        "            X = []\n",
        "            y = []\n",
        "            for batch_x, batch_y in client_data.as_numpy_iterator():\n",
        "                X.append(batch_x)\n",
        "                y.append(batch_y)\n",
        "            X = np.concatenate(X, axis=0)\n",
        "            y = np.concatenate(y, axis=0)\n",
        "            model.fit(X, y, epochs=num_epochs, verbose=0)\n",
        "            personalized_models.append(model)\n",
        "        logging.info(\"Personalized federated learning completed.\")\n",
        "        return personalized_models\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in personalized_federated_learning: %s\", e)\n",
        "        raise\n",
        "\n",
        "# Centralized and baseline models\n",
        "def train_centralized_model(X, y):\n",
        "    logging.info(\"Training centralized NN...\")\n",
        "    try:\n",
        "        model = create_model()\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "                      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                      metrics=['accuracy'])\n",
        "        history = model.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
        "        logging.info(\"Centralized NN trained.\")\n",
        "        return model, history\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in train_centralized_model: %s\", e)\n",
        "        raise\n",
        "\n",
        "def train_random_forest(X, y):\n",
        "    logging.info(\"Training Random Forest...\")\n",
        "    try:\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        model.fit(X, y)\n",
        "        logging.info(\"Random Forest trained.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in train_random_forest: %s\", e)\n",
        "        raise\n",
        "\n",
        "def train_local_models(client_data):\n",
        "    logging.info(\"Training local-only models...\")\n",
        "    try:\n",
        "        models = []\n",
        "        for X, y in client_data:\n",
        "            model = create_model()\n",
        "            model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "                          loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                          metrics=['accuracy'])\n",
        "            model.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
        "            models.append(model)\n",
        "        logging.info(\"Local-only models trained.\")\n",
        "        return models\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in train_local_models: %s\", e)\n",
        "        raise\n",
        "\n",
        "def evaluate_model(model, X, y, model_type='nn'):\n",
        "    logging.info(\"Evaluating %s model...\", model_type)\n",
        "    try:\n",
        "        if model_type == 'rf':\n",
        "            y_pred = model.predict(X)\n",
        "        else:\n",
        "            y_pred = (model.predict(X) > 0.5).astype(int).flatten()\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y, y_pred),\n",
        "            'precision': precision_score(y, y_pred),\n",
        "            'recall': recall_score(y, y_pred),\n",
        "            'f1': f1_score(y, y_pred),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
        "        }\n",
        "        logging.info(\"Evaluation completed for %s.\", model_type)\n",
        "        return metrics\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in evaluate_model: %s\", e)\n",
        "        raise\n",
        "\n",
        "# SHAP explanation function\n",
        "def compute_shap_explanations(model, X, model_type='nn', feature_names=None, subset_size=100, rng=None):\n",
        "    logging.info(\"Computing SHAP explanations for %s...\", model_type)\n",
        "    try:\n",
        "        if X.shape[1] != 30:\n",
        "            raise ValueError(f\"Expected 30 features in X, got {X.shape[1]}\")\n",
        "        X_subset = X[rng.choice(X.shape[0], min(subset_size, X.shape[0]), replace=False)]\n",
        "        logging.info(\"X_subset shape: %s\", X_subset.shape)\n",
        "\n",
        "        if model_type == 'rf':\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values_list = explainer.shap_values(X_subset)\n",
        "            shap_values = shap_values_list[1]\n",
        "        else:\n",
        "            # Use KernelExplainer for Keras models\n",
        "            def predict_proba(inputs):\n",
        "                inputs = tf.cast(inputs, tf.float32)\n",
        "                return model(inputs, training=False).numpy().flatten()\n",
        "\n",
        "            explainer = shap.KernelExplainer(predict_proba, X_subset)\n",
        "            shap_values = explainer.shap_values(X_subset)\n",
        "\n",
        "        logging.info(\"SHAP values shape: %s\", np.array(shap_values).shape)\n",
        "        if shap_values.shape[1] != 30:\n",
        "            raise ValueError(f\"Expected 30 features in shap_values, got {shap_values.shape[1]}\")\n",
        "        return shap_values, X_subset, explainer\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in compute_shap_explanations: %s\", e)\n",
        "        raise\n",
        "\n",
        "# Plot SHAP summary\n",
        "def plot_shap_summary(shap_values, X_subset, feature_names, model_name, save_path=None):\n",
        "    logging.info(\"Generating SHAP summary plot for %s...\", model_name)\n",
        "    try:\n",
        "        if len(feature_names) != 30:\n",
        "            raise ValueError(f\"Expected 30 feature names, got {len(feature_names)}\")\n",
        "        if shap_values.shape[1] != X_subset.shape[1]:\n",
        "            raise ValueError(f\"SHAP values shape {shap_values.shape} does not match X_subset shape {X_subset.shape}\")\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        shap.summary_plot(shap_values, X_subset, feature_names=feature_names, show=False)\n",
        "        plt.title(f\"SHAP Summary Plot for {model_name}\")\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        shap.summary_plot(shap_values, X_subset, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
        "        plt.title(f\"SHAP Feature Importance for {model_name}\")\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path.replace(\".png\", \"_bar.png\"))\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in plot_shap_summary: %s\", e)\n",
        "        raise\n",
        "\n",
        "# Main experiment\n",
        "def main():\n",
        "    rng = set_seed(42)\n",
        "    logging.info(\"Starting main function...\")\n",
        "    try:\n",
        "        # Load and split data\n",
        "        X, y, scaler, feature_names = load_and_preprocess_data()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        logging.info(\"Data split into train/test.\")\n",
        "\n",
        "        # Create IID and Non-IID partitions\n",
        "        client_data_iid = create_data_partitions(X_train, y_train, non_iid=False, rng=rng)\n",
        "        client_data_non_iid = create_data_partitions(X_train, y_train, non_iid=True, rng=rng)\n",
        "        client_datasets_iid = create_tff_datasets(client_data_iid)\n",
        "        client_datasets_non_iid = create_tff_datasets(client_data_non_iid)\n",
        "        logging.info(\"IID and Non-IID datasets ready.\")\n",
        "\n",
        "        # Train federated models\n",
        "        logging.info(\"Training Federated Model (IID)...\")\n",
        "        state_iid, metrics_iid = federated_training(client_datasets_iid, rng=rng)\n",
        "        logging.info(\"Training Federated Model (Non-IID)...\")\n",
        "        state_non_iid, metrics_non_iid = federated_training(client_datasets_non_iid, rng=rng)\n",
        "\n",
        "        # Apply global model weights\n",
        "        global_model_iid = create_model()\n",
        "        global_model_iid = apply_global_weights(state_iid, global_model_iid)\n",
        "        global_model_non_iid = create_model()\n",
        "        global_model_non_iid = apply_global_weights(state_non_iid, global_model_non_iid)\n",
        "\n",
        "        # Train personalized models\n",
        "        logging.info(\"Training Personalized Models (Non-IID)...\")\n",
        "        personalized_models = personalized_federated_learning(state_non_iid, client_datasets_non_iid)\n",
        "\n",
        "        # Train baseline models\n",
        "        centralized_nn, centralized_nn_history = train_centralized_model(X_train, y_train)\n",
        "        random_forest = train_random_forest(X_train, y_train)\n",
        "        centralized_pooled_nn, centralized_pooled_nn_history = train_centralized_model(X_train, y_train)\n",
        "        local_models = train_local_models(client_data_non_iid)\n",
        "\n",
        "        # Evaluate models\n",
        "        results = {}\n",
        "        results['Federated NN (IID)'] = evaluate_model(global_model_iid, X_test, y_test)\n",
        "        results['Federated NN (Non-IID)'] = evaluate_model(global_model_non_iid, X_test, y_test)\n",
        "        results['Centralized NN'] = evaluate_model(centralized_nn, X_test, y_test)\n",
        "        results['Random Forest'] = evaluate_model(random_forest, X_test, y_test, model_type='rf')\n",
        "        results['Centralized Pooled NN'] = evaluate_model(centralized_pooled_nn, X_test, y_test)\n",
        "        for i, model in enumerate(local_models):\n",
        "            results[f'Local-Only NN {i+1}'] = evaluate_model(model, X_test, y_test)\n",
        "        for i, model in enumerate(personalized_models):\n",
        "            results[f'Personalized NN {i+1}'] = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "        # Print evaluation results\n",
        "        for model_name, metrics in results.items():\n",
        "            logging.info(\"\\n%s:\", model_name)\n",
        "            logging.info(\"Accuracy: %.4f\", metrics['accuracy'])\n",
        "            logging.info(\"Precision: %.4f\", metrics['precision'])\n",
        "            logging.info(\"Recall: %.4f\", metrics['recall'])\n",
        "            logging.info(\"F1-Score: %.4f\", metrics['f1'])\n",
        "            logging.info(\"Confusion Matrix:\\n%s\", metrics['confusion_matrix'])\n",
        "\n",
        "        # SHAP explanations for important models\n",
        "        logging.info(\"Computing SHAP Explanations for important models...\")\n",
        "        shap_values_dict = {}\n",
        "        X_subset_dict = {}\n",
        "        explainer_dict = {}\n",
        "\n",
        "        # Federated NN (IID)\n",
        "        shap_values_global_iid, X_test_subset_iid, explainer_iid = compute_shap_explanations(\n",
        "            global_model_iid, X_test, model_type='nn', feature_names=feature_names, rng=rng)\n",
        "        shap_values_dict['Federated NN (IID)'] = shap_values_global_iid\n",
        "        X_subset_dict['Federated NN (IID)'] = X_test_subset_iid\n",
        "        explainer_dict['Federated NN (IID)'] = explainer_iid\n",
        "        plot_shap_summary(shap_values_global_iid, X_test_subset_iid, feature_names,\n",
        "                         \"Federated Model (IID)\", save_path=\"shap_fed_iid.png\")\n",
        "\n",
        "        # Federated NN (Non-IID)\n",
        "        shap_values_global_non_iid, X_test_subset_non_iid, explainer_non_iid = compute_shap_explanations(\n",
        "            global_model_non_iid, X_test, model_type='nn', feature_names=feature_names, rng=rng)\n",
        "        shap_values_dict['Federated NN (Non-IID)'] = shap_values_global_non_iid\n",
        "        X_subset_dict['Federated NN (Non-IID)'] = X_test_subset_non_iid\n",
        "        explainer_dict['Federated NN (Non-IID)'] = explainer_non_iid\n",
        "        plot_shap_summary(shap_values_global_non_iid, X_test_subset_non_iid, feature_names,\n",
        "                         \"Federated Model (Non-IID)\", save_path=\"shap_fed_non_iid.png\")\n",
        "\n",
        "        # Centralized NN\n",
        "        shap_values_centralized_nn, X_test_subset_centralized_nn, explainer_centralized = compute_shap_explanations(\n",
        "            centralized_nn, X_test, model_type='nn', feature_names=feature_names, rng=rng)\n",
        "        shap_values_dict['Centralized NN'] = shap_values_centralized_nn\n",
        "        X_subset_dict['Centralized NN'] = X_test_subset_centralized_nn\n",
        "        explainer_dict['Centralized NN'] = explainer_centralized\n",
        "        plot_shap_summary(shap_values_centralized_nn, X_test_subset_centralized_nn, feature_names,\n",
        "                         \"Centralized NN\", save_path=\"shap_centralized_nn.png\")\n",
        "\n",
        "        model_name = 'Federated NN (IID)'\n",
        "        shap_values = shap_values_dict[model_name]\n",
        "        X_subset = X_subset_dict[model_name]\n",
        "        explainer = explainer_dict[model_name]\n",
        "        feature_names = feature_names\n",
        "\n",
        "        num_instances_to_plot = 5\n",
        "        if X_subset.shape[0] >= num_instances_to_plot:\n",
        "            instance_indices = rng.choice(X_subset.shape[0], num_instances_to_plot, replace=False)\n",
        "        else:\n",
        "            instance_indices = np.arange(X_subset.shape[0])\n",
        "            print(f\"Warning: Subset size is less than {num_instances_to_plot}. Plotting all {X_subset.shape[0]} instances.\")\n",
        "\n",
        "\n",
        "        shap.initjs()\n",
        "\n",
        "        # Visualize SHAP explanations for each selected instance\n",
        "        print(f\"\\nSHAP Force Plots for {model_name} (Individual Instances):\")\n",
        "        for i in instance_indices:\n",
        "            print(f\"\\nInstance {i+1}:\")\n",
        "            display(shap.force_plot(explainer.expected_value, shap_values[i], X_subset[i], feature_names=feature_names))\n",
        "\n",
        "\n",
        "        # Training curves\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(metrics_iid['loss'], label='IID Loss')\n",
        "        plt.plot(metrics_non_iid['loss'], label='Non-IID Loss')\n",
        "        plt.title('Training Loss')\n",
        "        plt.xlabel('Round')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(metrics_iid['accuracy'], label='IID Accuracy')\n",
        "        plt.plot(metrics_non_iid['accuracy'], label='Non-IID Accuracy')\n",
        "        plt.title('Training Accuracy')\n",
        "        plt.xlabel('Round')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "\n",
        "        full_image_path = \"/content/training_curves.png\"\n",
        "        plt.savefig(full_image_path)\n",
        "        plt.show()\n",
        "\n",
        "        if os.path.exists(full_image_path):\n",
        "            with open(full_image_path, 'rb') as f:\n",
        "                image_data = f.read()\n",
        "            print(\"Training Curves:\")\n",
        "            display(Image(data=image_data))\n",
        "        else:\n",
        "            print(f\"Error: The image file '{full_image_path}' was not found.\")\n",
        "\n",
        "\n",
        "        return results, metrics_iid, metrics_non_iid, feature_names, X_test, y_test, global_model_iid, global_model_non_iid, centralized_nn, random_forest, shap_values_dict, X_subset_dict, explainer_dict, centralized_nn_history\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in main: %s\", e)\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(\"Script loaded successfully.\")\n",
        "    logging.info(\"Triggering main function...\")\n",
        "    results, metrics_iid, metrics_non_iid, feature_names, X_test, y_test, global_model_iid, global_model_non_iid, centralized_nn, random_forest, shap_values_dict, X_subset_dict, explainer_dict, centralized_nn_history = main()"
      ],
      "metadata": {
        "id": "6wYrUc2pYMRn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6e8480e1"
      },
      "source": [
        "# Visualize the confusion matrices\n",
        "print(\"\\nConfusion Matrix Heatmaps:\")\n",
        "for model_name, metrics in results.items():\n",
        "    cm = metrics['confusion_matrix']\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "    plt.title(f'{model_name} Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3573569b",
        "collapsed": true
      },
      "source": [
        "# Display the DataFrame with performance metrics\n",
        "print(\"\\nModel Performance Metrics:\")\n",
        "display(results_df_display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fcb1c11b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Training Curves:\")\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(metrics_iid['loss'], label='IID Loss')\n",
        "plt.plot(metrics_non_iid['loss'], label='Non-IID Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Round')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(metrics_iid['accuracy'], label='IID Accuracy')\n",
        "plt.plot(metrics_non_iid['accuracy'], label='Non-IID Accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Round')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPerformance Metrics Comparison:\")\n",
        "\n",
        "models_to_compare = ['Federated NN (IID)', 'Federated NN (Non-IID)', 'Centralized NN', 'Random Forest']\n",
        "metrics_to_plot = ['accuracy', 'precision', 'f1']\n",
        "\n",
        "comparison_data = results_df_display.loc[models_to_compare, metrics_to_plot]\n",
        "\n",
        "# Plotting the bar chart\n",
        "ax = comparison_data.plot(kind='bar', figsize=(12, 7))\n",
        "plt.title('Comparison of Model Performance Metrics')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Metric')\n",
        "plt.tight_layout()\n",
        "\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.3f')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "2d31fd54"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "models_to_compare = ['Federated NN (IID)', 'Federated NN (Non-IID)', 'Centralized NN', 'Random Forest']\n",
        "\n",
        "recall_data = results_df_display.loc[models_to_compare, ['recall']]\n",
        "\n",
        "# Plotting the bar chart for recall\n",
        "ax = recall_data.plot(kind='bar', figsize=(10, 6), legend=False)\n",
        "plt.title('Comparison of Model Recall Scores')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Recall')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.3f')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "eeac2db4"
      },
      "source": [
        "import shap\n",
        "\n",
        "# Force Plot\n",
        "model_name_to_analyze = 'Federated NN (IID)'\n",
        "\n",
        "shap_values = shap_values_dict.get(model_name_to_analyze)\n",
        "X_subset = X_subset_dict.get(model_name_to_analyze)\n",
        "explainer = explainer_dict.get(model_name_to_analyze)\n",
        "\n",
        "if shap_values is None or X_subset is None or explainer is None:\n",
        "    print(f\"SHAP data not found for model: {model_name_to_analyze}\")\n",
        "else:\n",
        "    num_instances_to_plot = 5\n",
        "    if X_subset.shape[0] >= num_instances_to_plot:\n",
        "        instance_indices = np.random.choice(X_subset.shape[0], num_instances_to_plot, replace=False)\n",
        "    else:\n",
        "        instance_indices = np.arange(X_subset.shape[0])\n",
        "        print(f\"Warning: Subset size is less than {num_instances_to_plot}. Plotting all {X_subset.shape[0]} instances.\")\n",
        "\n",
        "    shap.initjs()\n",
        "\n",
        "    print(f\"\\nSHAP Force Plots for {model_name_to_analyze} (Individual Instances):\")\n",
        "    for i in instance_indices:\n",
        "        print(f\"\\nInstance {i+1}:\")\n",
        "        display(shap.force_plot(explainer.expected_value, shap_values[i], X_subset[i], feature_names=feature_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "f246a499"
      },
      "source": [
        "# SHAP Dependence Plot\n",
        "model_name_to_analyze = 'Federated NN (IID)'\n",
        "\n",
        "shap_values = shap_values_dict.get(model_name_to_analyze)\n",
        "X_subset = X_subset_dict.get(model_name_to_analyze)\n",
        "\n",
        "if shap_values is None or X_subset is None:\n",
        "    print(f\"SHAP data not found for model: {model_name_to_analyze}\")\n",
        "else:\n",
        "    mean_abs_shap_values = np.mean(np.abs(shap_values), axis=0)\n",
        "    sorted_feature_indices = np.argsort(mean_abs_shap_values)[::-1]\n",
        "\n",
        "    num_top_features = 5\n",
        "    top_feature_indices = sorted_feature_indices[:num_top_features]\n",
        "    top_feature_names = [feature_names[i] for i in top_feature_indices]\n",
        "\n",
        "    print(f\"\\nGenerating and saving SHAP Dependence Plots for the top {num_top_features} features of {model_name_to_analyze}:\")\n",
        "\n",
        "    # Generate and display dependence plots for the top features\n",
        "    shap.initjs()\n",
        "    for i in top_feature_indices:\n",
        "        feature_name = feature_names[i].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
        "        plot_filename = f\"shap_dependence_plot_{model_name_to_analyze.replace(' ', '_').replace('(', '').replace(')', '')}_{feature_name}.png\"\n",
        "\n",
        "        print(f\"\\nGenerating dependence plot for feature: {feature_names[i]}\")\n",
        "        shap.dependence_plot(i, shap_values, X_subset, feature_names=feature_names, show=False)\n",
        "        plt.title(f'SHAP Dependence Plot for {feature_names[i]} ({model_name_to_analyze})')\n",
        "\n",
        "        plt.savefig(plot_filename, bbox_inches='tight')\n",
        "        print(f\"Plot saved to: {plot_filename}\")\n",
        "\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "142713c2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Training Loss vs Accuracy graph\n",
        "centralized_loss = centralized_nn_history.history['loss']\n",
        "centralized_accuracy = centralized_nn_history.history['accuracy']\n",
        "\n",
        "federated_iid_loss = metrics_iid['loss']\n",
        "federated_iid_accuracy = metrics_iid['accuracy']\n",
        "federated_non_iid_loss = metrics_non_iid['loss']\n",
        "federated_non_iid_accuracy = metrics_non_iid['accuracy']\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(centralized_loss, label='Centralized NN Loss')\n",
        "plt.plot(federated_iid_loss, label='Federated NN (IID) Loss')\n",
        "plt.plot(federated_non_iid_loss, label='Federated NN (Non-IID) Loss')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.xlabel('Epoch/Round')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(centralized_accuracy, label='Centralized NN Accuracy')\n",
        "plt.plot(federated_iid_accuracy, label='Federated NN (IID) Accuracy')\n",
        "plt.plot(federated_non_iid_accuracy, label='Federated NN (Non-IID) Accuracy')\n",
        "plt.title('Training Accuracy Comparison')\n",
        "plt.xlabel('Epoch/Round')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ec39552",
        "collapsed": true
      },
      "source": [
        "# Filter the DataFrame to show only incorrect predictions\n",
        "incorrect_predictions_df = prediction_results_df[prediction_results_df['Actual Label'] != prediction_results_df['Predicted Label']]\n",
        "\n",
        "print(\"Instances with Incorrect Predictions (Federated NN Non-IID):\")\n",
        "if not incorrect_predictions_df.empty:\n",
        "    display(incorrect_predictions_df)\n",
        "else:\n",
        "    print(\"No incorrect predictions found for this model on the test set.\")\n",
        "\n",
        "false_positives_df = incorrect_predictions_df[incorrect_predictions_df['Predicted Label'] == 'malignant']\n",
        "false_negatives_df = incorrect_predictions_df[incorrect_predictions_df['Predicted Label'] == 'benign']\n",
        "\n",
        "print(\"\\nFalse Positives:\")\n",
        "if not false_positives_df.empty:\n",
        "    display(false_positives_df)\n",
        "else:\n",
        "    print(\"No False Positives found.\")\n",
        "\n",
        "print(\"\\nFalse Negatives:\")\n",
        "if not false_negatives_df.empty:\n",
        "    display(false_negatives_df)\n",
        "else:\n",
        "    print(\"No False Negatives found.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ff43794",
        "collapsed": true
      },
      "source": [
        "# Updated performance table\n",
        "models_for_table = ['Centralized NN', 'Federated NN (Non-IID)', 'Federated NN (IID)', 'Random Forest']\n",
        "\n",
        "updated_performance_table = results_df_display.loc[models_for_table, ['accuracy', 'precision', 'recall', 'f1']].copy()\n",
        "\n",
        "updated_performance_table.loc['Personalized FL-NN (Avg)'] = average_personalized_metrics[['accuracy', 'precision', 'recall', 'f1']]\n",
        "updated_performance_table.loc['Local-Only NNs (Avg)'] = average_local_only_metrics[['accuracy', 'precision', 'recall', 'f1']]\n",
        "\n",
        "print(\"Updated Model Performance Metrics Table:\")\n",
        "display(updated_performance_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9192e814",
        "collapsed": true
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Confusion matrix visualization\n",
        "models_to_visualize = ['Centralized NN', 'Federated NN (Non-IID)', 'Federated NN (IID)', 'Random Forest']\n",
        "\n",
        "print(\"\\nConfusion Matrix Heatmaps (Including Averages):\")\n",
        "\n",
        "# Visualize confusion matrices for individual models\n",
        "for model_name in models_to_visualize:\n",
        "    if model_name in results and 'confusion_matrix' in results[model_name]:\n",
        "        cm = results[model_name]['confusion_matrix']\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                    xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                    yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "        plt.title(f'{model_name} Confusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Confusion matrix data not found for {model_name}\")\n",
        "\n",
        "personalized_cms = [results[name]['confusion_matrix'] for name in results if name.startswith('Personalized NN') and 'confusion_matrix' in results[name]]\n",
        "if personalized_cms:\n",
        "    average_personalized_cm = np.mean(personalized_cms, axis=0)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(average_personalized_cm, annot=True, fmt='.1f', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "    plt.title('Personalized FL-NN (Avg) Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No confusion matrix data found for Personalized NN models.\")\n",
        "\n",
        "\n",
        "local_only_cms = [results[name]['confusion_matrix'] for name in results if name.startswith('Local-Only NN') and 'confusion_matrix' in results[name]]\n",
        "if local_only_cms:\n",
        "    average_local_only_cm = np.mean(local_only_cms, axis=0)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(average_local_only_cm, annot=True, fmt='.1f', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "    plt.title('Local-Only NNs (Avg) Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No confusion matrix data found for Local-Only NN models.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f86949e",
        "collapsed": true
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# including ROC-AUC in metrics table\n",
        "def evaluate_model_with_roc_auc(model, X, y, model_type='nn'):\n",
        "    logging.info(\"Evaluating %s model...\", model_type)\n",
        "    try:\n",
        "        if model_type == 'rf':\n",
        "            y_pred_proba = model.predict_proba(X)[:, 1]\n",
        "            y_pred = model.predict(X)\n",
        "        else:\n",
        "            y_pred_proba = model.predict(X).flatten()\n",
        "            y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y, y_pred),\n",
        "            'precision': precision_score(y, y_pred),\n",
        "            'recall': recall_score(y, y_pred),\n",
        "            'f1': f1_score(y, y_pred),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred),\n",
        "            'roc_auc': roc_auc_score(y, y_pred_proba)\n",
        "        }\n",
        "        logging.info(\"Evaluation completed for %s.\", model_type)\n",
        "        return metrics\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in evaluate_model_with_roc_auc: %s\", e)\n",
        "        raise\n",
        "\n",
        "results_with_roc_auc = {}\n",
        "\n",
        "try:\n",
        "    X, y, scaler, feature_names = load_and_preprocess_data()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    logging.info(\"Data split into train/test within evaluation cell.\")\n",
        "\n",
        "    rng = set_seed(42)\n",
        "    client_data_non_iid = create_data_partitions(X_train, y_train, non_iid=True, rng=rng)\n",
        "    client_datasets_non_iid = create_tff_datasets(client_data_non_iid)\n",
        "    logging.info(\"Client data and datasets re-created within evaluation cell.\")\n",
        "\n",
        "    #Re-training models to ensure availablability\n",
        "    logging.info(\"Re-training Federated Model (IID) within evaluation cell...\")\n",
        "    state_iid, _ = federated_training(create_tff_datasets(create_data_partitions(X_train, y_train, non_iid=False, rng=rng)), rng=rng)\n",
        "    global_model_iid = create_model()\n",
        "    global_model_iid = apply_global_weights(state_iid, global_model_iid)\n",
        "\n",
        "    logging.info(\"Re-training Federated Model (Non-IID) within evaluation cell...\")\n",
        "    state_non_iid, _ = federated_training(client_datasets_non_iid, rng=rng)\n",
        "    global_model_non_iid = create_model()\n",
        "    global_model_non_iid = apply_global_weights(state_non_iid, global_model_non_iid)\n",
        "\n",
        "    centralized_nn, _ = train_centralized_model(X_train, y_train)\n",
        "    random_forest = train_random_forest(X_train, y_train)\n",
        "    centralized_pooled_nn_re_trained, _ = train_centralized_model(X_train, y_train)\n",
        "\n",
        "    local_models_re_trained = train_local_models(client_data_non_iid)\n",
        "\n",
        "    personalized_models_re_trained = personalized_federated_learning(state_non_iid, client_datasets_non_iid)\n",
        "\n",
        "\n",
        "    results_with_roc_auc['Federated NN (IID)'] = evaluate_model_with_roc_auc(global_model_iid, X_test, y_test)\n",
        "    results_with_roc_auc['Federated NN (Non-IID)'] = evaluate_model_with_roc_auc(global_model_non_iid, X_test, y_test)\n",
        "\n",
        "    results_with_roc_auc['Centralized NN'] = evaluate_model_with_roc_auc(centralized_nn, X_test, y_test)\n",
        "    results_with_roc_auc['Random Forest'] = evaluate_model_with_roc_auc(random_forest, X_test, y_test, model_type='rf')\n",
        "    results_with_roc_auc['Centralized Pooled NN'] = evaluate_model_with_roc_auc(centralized_pooled_nn_re_trained, X_test, y_test)\n",
        "\n",
        "\n",
        "    for i, model in enumerate(local_models_re_trained):\n",
        "        results_with_roc_auc[f'Local-Only NN {i+1}'] = evaluate_model_with_roc_auc(model, X_test, y_test)\n",
        "\n",
        "    for i, model in enumerate(personalized_models_re_trained):\n",
        "        results_with_roc_auc[f'Personalized NN {i+1}'] = evaluate_model_with_roc_auc(model, X_test, y_test)\n",
        "\n",
        "\n",
        "    results_df_display_with_roc_auc = pd.DataFrame(results_with_roc_auc).T\n",
        "    results_df_display_with_roc_auc = results_df_display_with_roc_auc[['accuracy', 'precision', 'recall', 'f1', 'roc_auc']] # Define column order\n",
        "\n",
        "\n",
        "    personalized_models_df_with_roc_auc = results_df_display_with_roc_auc[results_df_display_with_roc_auc.index.str.startswith('Personalized NN')]\n",
        "\n",
        "    average_personalized_metrics_with_roc_auc = personalized_models_df_with_roc_auc[['accuracy', 'precision', 'recall', 'f1', 'roc_auc']].mean()\n",
        "\n",
        "\n",
        "    local_only_models_df_with_roc_auc = results_df_display_with_roc_auc[results_df_display_with_roc_auc.index.str.startswith('Local-Only NN')]\n",
        "\n",
        "    average_local_only_metrics_with_roc_auc = local_only_models_df_with_roc_auc[['accuracy', 'precision', 'recall', 'f1', 'roc_auc']].mean()\n",
        "\n",
        "\n",
        "    models_for_table_with_roc_auc = ['Centralized NN', 'Federated NN (Non-IID)', 'Federated NN (IID)', 'Random Forest']\n",
        "\n",
        "    updated_performance_table_with_roc_auc = results_df_display_with_roc_auc.loc[models_for_table_with_roc_auc, ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']].copy()\n",
        "\n",
        "    updated_performance_table_with_roc_auc.loc['Personalized FL-NN (Avg)'] = average_personalized_metrics_with_roc_auc[['accuracy', 'precision', 'recall', 'f1', 'roc_auc']]\n",
        "    updated_performance_table_with_roc_auc.loc['Local-Only NNs (Avg)'] = average_local_only_metrics_with_roc_auc[['accuracy', 'precision', 'recall', 'f1', 'roc_auc']]\n",
        "\n",
        "\n",
        "    print(\"\\nUpdated Model Performance Metrics Table (with ROC-AUC):\")\n",
        "    display(updated_performance_table_with_roc_auc)\n",
        "\n",
        "    results = results_with_roc_auc\n",
        "    results_df_display = results_df_display_with_roc_auc\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(\"An error occurred during re-evaluation: %s\", e)\n",
        "    print(f\"An error occurred during re-evaluation: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "650fae40",
        "collapsed": true
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Generating ROC curves...\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')\n",
        "\n",
        "models_to_plot_roc = {\n",
        "    'Federated NN (IID)': global_model_iid,\n",
        "    'Federated NN (Non-IID)': global_model_non_iid,\n",
        "    'Centralized NN': centralized_nn,\n",
        "    'Random Forest': random_forest\n",
        "}\n",
        "\n",
        "# Plot ROC curves for individual models\n",
        "for model_name, model in models_to_plot_roc.items():\n",
        "    if model_name == 'Random Forest':\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_pred_proba = model.predict(X_test).flatten()\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "if 'Personalized FL-NN (Avg)' in updated_performance_table_with_roc_auc.index:\n",
        "    avg_personalized_roc_auc = updated_performance_table_with_roc_auc.loc['Personalized FL-NN (Avg)', 'roc_auc']\n",
        "    plt.plot([], [], ' ', label=f'Personalized FL-NN (Avg) (AUC = {avg_personalized_roc_auc:.3f})')\n",
        "\n",
        "if 'Local-Only NNs (Avg)' in updated_performance_table_with_roc_auc.index:\n",
        "    avg_local_only_roc_auc = updated_performance_table_with_roc_auc.loc['Local-Only NNs (Avg)', 'roc_auc']\n",
        "    plt.plot([], [], ' ', label=f'Local-Only NNs (Avg) (AUC = {avg_local_only_roc_auc:.3f})')\n",
        "\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"ROC curve generation complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}